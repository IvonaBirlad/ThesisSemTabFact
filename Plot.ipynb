{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Sklearn metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#Pytorch and huggingface\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "## Adding the path of docformer to system path\n",
    "sys.path.append('./docformer/src/docformer/')\n",
    "# sys.path.append('/gpfs/home1/ibirlad/.conda/envs/semtabfact_venv/bin/tesseract')\n",
    "\n",
    "## Importing the functions from the DocFormer Repo\n",
    "from dataset import create_features\n",
    "from modeling import DocFormerEncoder, ResNetFeatureExtractor, DocFormerEmbeddings, LanguageFeatureExtractor, DocFormer\n",
    "from transformers import BertTokenizerFast"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\ivona\\Desktop\\semtabfactSnel\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (4.64.1)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (2023.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (1.24.2)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (1.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from evaluate) (0.14.1)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp39-cp39-win_amd64.whl (21.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ivona\\desktop\\semtabfactsnel\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: dill, xxhash, pyarrow, multiprocess, responses, datasets, evaluate\n",
      "Successfully installed datasets-2.13.0 dill-0.3.6 evaluate-0.4.0 multiprocess-0.70.14 pyarrow-12.0.1 responses-0.18.0 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install evaluate\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#Environment setup\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "## Global variables\n",
    "seed = 42\n",
    "target_size = (500, 384)\n",
    "n_classes = 3\n",
    "\n",
    "## Setting some hyperparameters\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class DocFormerForClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(DocFormerForClassification, self).__init__()\n",
    "\n",
    "        self.resnet = ResNetFeatureExtractor(hidden_dim=config['max_position_embeddings'])\n",
    "        self.embeddings = DocFormerEmbeddings(config)\n",
    "        self.lang_emb = LanguageFeatureExtractor()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config['hidden_dropout_prob'])\n",
    "        self.linear_layer = nn.Linear(in_features=config['hidden_size'], out_features=n_classes)  ## Number of Classes\n",
    "        self.encoder = DocFormerEncoder(config)\n",
    "        self.linear = nn.Linear(2*config['hidden_size'], config['hidden_size'])\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        x_feat = batch_dict['x_features']\n",
    "        y_feat = batch_dict['y_features']\n",
    "\n",
    "        table_token = batch_dict['input_ids']\n",
    "        statement_token = batch_dict['statement']\n",
    "        img = batch_dict['resized_scaled_img']\n",
    "        v_bar_s, t_bar_s = self.embeddings(x_feat, y_feat)\n",
    "        v_bar = self.resnet(img)\n",
    "        table_emb = self.lang_emb(table_token)\n",
    "        statement_emb = self.lang_emb(statement_token)\n",
    "        # t_bar = table_emb + statement_emb  #(4, 128, 768)\n",
    "        text = torch.cat([table_emb, statement_emb], dim=-1)\n",
    "        t_bar = self.linear(text)\n",
    "        # print(t_bar.shape)\n",
    "        # t_bar = t_bar.long()\n",
    "        out = self.encoder(t_bar, v_bar, t_bar_s, v_bar_s)\n",
    "        out = self.linear_layer(out)\n",
    "        out = out[:, 0, :]\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class DocFormer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config, model_name = \"docformer_base\", lr=1e-3):\n",
    "        super(DocFormer, self).__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "        self.docformer = DocFormerForClassification(config)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.num_classes = n_classes\n",
    "        self.train_accuracy_metric = torchmetrics.Accuracy(task='multiclass',\n",
    "                                                           num_classes=self.num_classes)\n",
    "        self.val_accuracy_metric = torchmetrics.Accuracy(task='multiclass',\n",
    "                                                         num_classes=self.num_classes)\n",
    "        self.f1_metric = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes)\n",
    "\n",
    "        self.label = []\n",
    "        self.logit = []\n",
    "\n",
    "    def forward(self, batch_dict):\n",
    "        logits = self.docformer(batch_dict)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self.forward(batch)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        ## Calculating the accuracy score\n",
    "        train_acc = self.train_accuracy_metric(preds, batch[\"label\"])\n",
    "\n",
    "        ## Logging\n",
    "        self.log('train/loss', loss, prog_bar=True, on_epoch=True, logger=True)\n",
    "        self.log('train/acc', train_acc, prog_bar=True, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self.forward(batch)\n",
    "        loss = nn.CrossEntropyLoss()(logits, batch['label'])\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        labels = batch['label']\n",
    "        # Metrics\n",
    "        valid_acc = self.val_accuracy_metric(preds, labels)\n",
    "        f1 = self.f1_metric(preds, labels)\n",
    "\n",
    "        # Logging metrics\n",
    "        self.log(\"valid/loss\", loss, prog_bar=True, on_step=True, logger=True)\n",
    "        self.log(\"valid/acc\", valid_acc, prog_bar=True, on_epoch=True, logger=True, on_step=True)\n",
    "        self.log(\"valid/f1\", f1, prog_bar=True, on_epoch=True)\n",
    "        self.label.append(batch['label'])\n",
    "        self.logit.append(logits)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # val_loss_mean = np.mean(self.training_losses)\n",
    "        labels = torch.cat(self.label)\n",
    "        logits = torch.cat(self.logit)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "        self.logger.experiment.log(\n",
    "            {\"roc\": wandb.plot.roc_curve(labels.cpu().numpy(), logits.cpu().numpy())})\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams['lr'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"coordinate_size\": 96,  ## (768/8), 8 for each of the 8 coordinates of x, y\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"hidden_size\": 768,\n",
    "    \"image_feature_pool_shape\": [7, 7, 256],\n",
    "    \"intermediate_ff_size_factor\": 4,\n",
    "    \"max_2d_position_embeddings\": 1000,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"max_relative_positions\": 8,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"shape_size\": 96,\n",
    "    \"vocab_size\": 30522,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"classes\": 3\n",
    "}\n",
    "\n",
    "docformer = DocFormer(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "model_path = \"C:/Users/ivona/Desktop/semtabfactSnel/models/epoch=8-step=1413.ckpt\"\n",
    "pl_model = docformer.load_from_checkpoint(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "## Importing the data\n",
    "ROOT_DIRECTORY_PATH = os.path.abspath(\"\")\n",
    "PNG_PATH_MAN = \"png_data/data_aug.csv\"\n",
    "PNG_PATH_AUTO = \"png_data_auto/data_aug_auto.csv\"\n",
    "PNG_PATH_TEST = \"png_data_test/data_test.csv\"\n",
    "\n",
    "#combine data from manual and automatic annotations\n",
    "data_man = pd.read_csv(os.path.join(ROOT_DIRECTORY_PATH, PNG_PATH_MAN), index_col=0)\n",
    "data_auto = pd.read_csv(os.path.join(ROOT_DIRECTORY_PATH, PNG_PATH_AUTO), index_col=0)\n",
    "data_test = pd.read_csv(os.path.join(ROOT_DIRECTORY_PATH, PNG_PATH_TEST), index_col=0)\n",
    "\n",
    "data_auto = shuffle(data_auto, random_state=42, n_samples=3872) #Only use 10000 samples in total for training time optimization\n",
    "data = pd.concat([data_man, data_auto])\n",
    "data = shuffle(data, random_state=42)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data[\"table_name\"] = ROOT_DIRECTORY_PATH + \"/\" + data[\"table_name\"]\n",
    "data_test[\"table_name\"] = ROOT_DIRECTORY_PATH + \"/\" + data_test[\"table_name\"]\n",
    "\n",
    "train_df = data.reset_index(drop=True)\n",
    "valid_df = data_test.reset_index(drop=True)\n",
    "\n",
    "## 3. Making the dataset\n",
    "\n",
    "## Creating the dataset\n",
    "\n",
    "class SemTabFactData(Dataset):\n",
    "\n",
    "    def __init__(self, image_list, label_list, statement_list, target_size, tokenizer, max_len=512, transform=None):\n",
    "\n",
    "        self.image_list = image_list\n",
    "        self.label_list = label_list\n",
    "        self.statement_list = statement_list\n",
    "        self.target_size = target_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        statement = self.statement_list[idx]\n",
    "\n",
    "        ## More on this, in the repo mentioned previously\n",
    "        final_encoding = create_features(\n",
    "            img_path,\n",
    "            self.tokenizer,\n",
    "            add_batch_dim=False,\n",
    "            target_size=self.target_size,\n",
    "            max_seq_length=self.max_len,\n",
    "            path_to_save=None,\n",
    "            save_to_disk=False,\n",
    "            apply_mask_for_mlm=False,\n",
    "            extras_for_debugging=False,\n",
    "            use_ocr=True  # Please provide the bounding box and words or pass the argument \"use_ocr\" = True\n",
    "        )\n",
    "        if self.transform is not None:\n",
    "            ## Note that, ToTensor is already applied on the image\n",
    "            final_encoding['resized_scaled_img'] = self.transform(final_encoding['resized_scaled_img'])\n",
    "\n",
    "        keys_to_reshape = ['x_features', 'y_features', 'resized_and_aligned_bounding_boxes']\n",
    "        for key in keys_to_reshape:\n",
    "            final_encoding[key] = final_encoding[key][:self.max_len]\n",
    "\n",
    "        final_encoding['label'] = torch.as_tensor(label).long()\n",
    "\n",
    "        statement_encoding = tokenizer(statement,\n",
    "                                       padding=\"max_length\",\n",
    "                                       max_length=self.max_len,\n",
    "                                       truncation=True)\n",
    "\n",
    "        final_encoding['statement'] = torch.as_tensor(statement_encoding[\"input_ids\"])\n",
    "        return final_encoding\n",
    "\n",
    "\n",
    "## Defining the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "## Normalization to these mean and std (I have seen some tutorials used this, and also in image reconstruction, so used it)\n",
    "transform = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "train_ds = SemTabFactData(train_df['table_name'].tolist(), train_df['label'].tolist(), train_df['statement'].tolist(),\n",
    "                          target_size, tokenizer, config['max_position_embeddings'], transform)\n",
    "val_ds = SemTabFactData(valid_df['table_name'].tolist(), valid_df['label'].tolist(), valid_df['statement'].tolist(),\n",
    "                        target_size, tokenizer, config['max_position_embeddings'], transform)\n",
    "\n",
    "### Collate Function:\n",
    "# from [here](https: // stackoverflow.com / questions / 65279115 / how - to - use - collate - fn - with-dataloaders)\n",
    "\n",
    "def collate_fn(data_bunch):\n",
    "    '''\n",
    "    A function for the dataloader to return a batch dict of given keys\n",
    "\n",
    "    data_bunch: List of dictionary\n",
    "    '''\n",
    "\n",
    "    dict_data_bunch = {}\n",
    "\n",
    "    for i in data_bunch:\n",
    "        for (key, value) in i.items():\n",
    "            if key not in dict_data_bunch:\n",
    "                dict_data_bunch[key] = []\n",
    "            dict_data_bunch[key].append(value)\n",
    "\n",
    "    for key in list(dict_data_bunch.keys()):\n",
    "        dict_data_bunch[key] = torch.stack(dict_data_bunch[key], axis=0)\n",
    "\n",
    "    return dict_data_bunch\n",
    "\n",
    "\n",
    "## 4. Defining the DataModule\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_dataset, val_dataset, batch_size=16):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          collate_fn=collate_fn, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "datamodule = DataModule(train_ds, val_ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "true_predictions = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3aff485f4594c4b9a293edfc3c8dd93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eval_metric = evaluate.load(\"precision\", average= \"micro\")\n",
    "# eval_metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "pl_model.eval();\n",
    "\n",
    "model = pl_model.to(device)\n",
    "\n",
    "for idx, batch in enumerate(tqdm(datamodule.val_dataloader())):\n",
    "    # move batch to device\n",
    "    batch = {k:v.to(device) for k,v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "      outputs = model.forward(batch)\n",
    "\n",
    "    predictions = outputs.argmax(-1)\n",
    "    true_predictions.append(predictions)\n",
    "    true_labels.append(batch[\"label\"])\n",
    "    eval_metric.add_batch(references=predictions, predictions=batch[\"label\"])\n",
    "    eval_metric.compute(average= \"micro\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3660030627871363}\n",
      "{'recall': 0.3660030627871363}\n",
      "{'f1': 0.3660030627871363}\n"
     ]
    }
   ],
   "source": [
    "for key in ['precision', 'recall', 'f1']:\n",
    "    eval_metric = evaluate.load(key, average = \"micro\")\n",
    "    for i in range(len(true_labels)):\n",
    "        eval_metric.add_batch(references=true_labels[i], predictions=true_predictions[i])\n",
    "    print(eval_metric.compute(average = \"micro\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
